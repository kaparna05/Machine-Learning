# -*- coding: utf-8 -*-
"""Heart_Disease_Prediction_CaseStudy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KYFFR3yzPqw2odgs4Ge4jV1Yf3vHwBpF

#**Framingham Heart Study (FHS)**
"""

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('framingham.csv')

df.head()

"""#Exploratory Data Analysis

Step 1: Handling missing values
"""

plt.figure(figsize=(12,12))
sns.heatmap(df.corr(),annot=True)

df.shape

df.info()

df.isnull().sum()

df['education'].value_counts()

df['education'].unique()

105+29+53+50+19+1+388

(645/4238)*100

#As education is a categorical value, we will replace the missing values with mode value

df['education'].fillna(1,inplace=True)

df['education'].unique()

df['education'].value_counts()

print("Mean: ",df['cigsPerDay'].mean())
print("Median: ",df['cigsPerDay'].median())

df['cigsPerDay'].unique()

sns.displot(df.isnull(), x="cigsPerDay",hue='TenYearCHD')
plt.show()

"""As the median value of the cigsPerDay is 0, and the corresponding heart disease chances are very less, replacing it my median value"""

df['cigsPerDay'].fillna(df['cigsPerDay'].median(),inplace=True)

df.isnull().sum()

df['BPMeds'].value_counts()

print("Mode: ",df['BPMeds'].mode())

df['BPMeds'].fillna(0,inplace=True)

print("Mean: ",df['totChol'].mean())
print("Median: ",df['totChol'].median())

df['totChol'].fillna(df['totChol'].mean(),inplace=True)

print("Mean: ",df['BMI'].mean())
print("Median: ",df['BMI'].median())

df['BMI'].fillna(df['BMI'].mean(),inplace=True)

print("Mean: ",df['heartRate'].mean())
print("Median: ",df['heartRate'].median())

df['heartRate'].fillna(df['heartRate'].mean(),inplace=True)

print("Mean: ",df['glucose'].mean())
print("Median: ",df['glucose'].median())

df['glucose'].fillna(df['glucose'].mean(),inplace=True)

df.isnull().sum()

"""Missing values of all the columns has been handled by filling them my their respective mean/median/mode as per the data type and data analysis

Step 2: Handling Outliers
"""

plt.figure(figsize=(10,7))
sns.boxplot(data=df,x='totChol',whis=3)
plt.show()

plt.figure(figsize=(10,7))
sns.boxplot(data=df,x='sysBP',whis=3)
plt.show()

plt.figure(figsize=(10,7))
sns.boxplot(data=df,x='diaBP',whis=3)
plt.show()

plt.figure(figsize=(10,7))
sns.boxplot(data=df,x='BMI',whis=3)
plt.show()

plt.figure(figsize=(10,7))
sns.boxplot(data=df,x='heartRate',whis=3)
plt.show()

plt.figure(figsize=(10,7))
sns.boxplot(data=df,x='glucose',whis=3)
plt.show()

"""Post data analysis, we have considered heartRate as a parameter to exclude Outliers"""

df[df['heartRate']>125]

df.drop([339,358,3142],inplace=True)

plt.figure(figsize=(10,7))
sns.boxplot(data=df,x='heartRate',whis=3)
plt.show()

"""All the outliers has been handled wrt heartRate

Split numerical and categorical data
"""

df.info()

df.head()

#categorical columns -- male, education, currentSmoker, BPMeds, TenYearCHD, prevalentStroke, prevalentHyp, diabetes,TenYearCHD  
#numerical columns   -- age, cigsPerDay, totChol, sysBP, diaBP, BMI, heartRate, glucose

df_num = df[['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']]
df_cat = df[['male', 'education', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes','TenYearCHD']]

"""Handling Skewness """

from scipy.stats import skew

for col in df_num:
  try:
    print(col,"=",skew(df_num[col]))
    sns.distplot(df_num[col])
    plt.show()
  except:
    pass
  finally:
    print("**********************************************")

#cigsPerDay,totChol,sysBP, diaBP, BMI, glucose features have positive skewness

plt.figure(figsize=(12,12))
sns.heatmap(df.corr(),annot=True)

#Reducing skewness for all the columns as there is no high correlation between the features and target variable
##cigsPerDay,totChol,sysBP, diaBP, BMI, glucose features have positive skewness

print(min(df['cigsPerDay']))
print(max(df['cigsPerDay']))

print(min(df['totChol']))
print(max(df['totChol']))

print(min(df['sysBP']))
print(max(df['sysBP']))

print(min(df['diaBP']))
print(max(df['diaBP']))

print(min(df['BMI']))
print(max(df['BMI']))

print(min(df['glucose']))
print(max(df['glucose']))

#None of the column has negative value, hence we can handle skewness for all the columns

skewed_data = np.sqrt(df_num['cigsPerDay'])
skew(skewed_data)

skewed_data = np.sqrt(df_num['totChol'])
skew(skewed_data)

skewed_data = np.log(df_num['sysBP'])
skew(skewed_data)

skewed_data = np.sqrt(df_num['diaBP'])
skew(skewed_data)

skewed_data = np.sqrt(df_num['BMI'])
skew(skewed_data)

skewed_data = np.log10(df_num['glucose'])
skew(skewed_data)

"""Column glucose has highly skewed data. As it has a higher correlation with diabetes, considering diabetes feature for model prediction and hence excluding glucose from dataset."""

df_num.drop('glucose',axis=1,inplace=True)

df_num.head()

df_num['cigsPerDay'] = np.sqrt(df_num['cigsPerDay'])
df_num['totChol'] = np.sqrt(df_num['totChol'])
df_num['sysBP'] = np.log(df_num['sysBP'])
df_num['diaBP'] = np.sqrt(df_num['diaBP'])
df_num['BMI'] = np.sqrt(df_num['BMI'])

df_num.head()

"""As the columns cannot have negative values, performing min-max scaler"""

from sklearn.preprocessing import MinMaxScaler
for col in df_num:
  mm = MinMaxScaler()
  df_num[col] = mm.fit_transform(df_num[[col]])
df_num.head()

df_new = pd.concat([df_num,df_cat],axis=1)

df_new.head()

"""#Modeling & Feature Selection"""

from sklearn.model_selection import train_test_split

x = df_new.drop('TenYearCHD',axis=1)
y = df_new['TenYearCHD']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)

from sklearn.linear_model import LogisticRegression

logr = LogisticRegression()
logr.fit(x_train,y_train)

y_hat = logr.predict(x_test)

from sklearn.metrics import confusion_matrix
tn, fp, fn, tp = confusion_matrix(y_test,y_hat).ravel()

print(tp,",",fn)
print(fp,",",tn)

from sklearn.metrics import(accuracy_score, recall_score, precision_score, f1_score)

print("Accuracy Score: ",accuracy_score(y_test, y_hat))
print("Recall Score: ",recall_score(y_test, y_hat))
print("Precision Score: ",precision_score(y_test, y_hat))
print("F1 Score: ",f1_score(y_test, y_hat))

"""ROC-AUC Curve"""

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test,y_hat))

from sklearn.metrics import roc_curve
fpr, tpr, threshold = roc_curve(y_test,y_hat)

plt.plot(fpr,tpr,'r-',label="Logistic Model")
plt.xlabel("False positive rate")
plt.ylabel("True positive rate")
plt.legend()
plt.show()

"""Decision Tree Classification"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(x_train,y_train)
dt.score(x_test,y_test)

from sklearn import tree

dt1 = DecisionTreeClassifier(criterion="entropy")
dt1.fit(x_train,y_train)
dt1.score(x_test,y_test)

tree.plot_tree(dt.fit(x_train,y_train),fontsize=6)

tree.plot_tree(dt1.fit(x_train,y_train),fontsize=6)

"""Performing ANNOVA Test

"""

from sklearn.feature_selection import f_regression

from sklearn.feature_selection import SelectKBest
annova = SelectKBest(score_func=f_regression,k=10)
annova.fit(x_train,y_train)
x_train_annova = annova.transform(x_train)
x_test_annova = annova.transform(x_test)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(x_train_annova, y_train)
print("Bias = ",lr.score(x_train_annova,y_train))
print("Variance = ",lr.score(x_test_annova,y_test))

scores_df = pd.DataFrame(annova.scores_)
columns_df = pd.DataFrame(x.columns)

featureScore = pd.concat([columns_df,scores_df],axis=1)
featureScore.columns = ['Feature Name','Score']

featureScore

annova = SelectKBest(score_func=f_regression,k=10)
annova.fit(x_train,y_train)
x_train_decision_annova = annova.transform(x_train)
x_test_decision_annova = annova.transform(x_test)

dt = DecisionTreeClassifier()
dt.fit(x_train_decision_annova, y_train)
print("Bias = ",dt.score(x_train_decision_annova,y_train))
print("Variance = ",dt.score(x_test_decision_annova,y_test))

"""Decision Tree Pruning"""

annova = SelectKBest(score_func=f_regression,k=10)
annova.fit(x_train,y_train)
x_train_decision_annova = annova.transform(x_train)
x_test_decision_annova = annova.transform(x_test)

dt = DecisionTreeClassifier(max_depth=5)
dt.fit(x_train_decision_annova, y_train)
print("Bias = ",dt.score(x_train_decision_annova,y_train))
print("Variance = ",dt.score(x_test_decision_annova,y_test))

annova = SelectKBest(score_func=f_regression,k=10)
annova.fit(x_train,y_train)
x_train_decision_annova = annova.transform(x_train)
x_test_decision_annova = annova.transform(x_test)

dt = DecisionTreeClassifier(min_samples_leaf=100)
dt.fit(x_train_decision_annova, y_train)
print("Bias = ",dt.score(x_train_decision_annova,y_train))
print("Variance = ",dt.score(x_test_decision_annova,y_test))

"""**As Decision Tree Classifier gives us an Bias value 1 and also has a huge difference between Bias and Variance, Logistic Regression performs better modelling for the case study with 85% score**"""