# -*- coding: utf-8 -*-
"""Airbnb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YEJmfmOS6OAh6PB94hDYcJxIQvodG7wB
"""

import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split #Function
from sklearn.linear_model import LinearRegression #Class
from sklearn.metrics import mean_squared_error, r2_score #function

df = pd.read_csv('AB_NYC_2019.csv')
df.head(10)

df.shape

df.info()

"""As per analysis id,name,neighbourhood are having unique values so need not be considered.
Also host_name is having host_id so keeping host_id and removing host_name
Also latitude and longitude will not affect the price
"""

df.drop(['id','name','neighbourhood','host_name','latitude','longitude'],axis=1,inplace=True)

"""WE will start with EDA. 
1. Handling missing values
"""

df.isnull().sum()

"""last_review is categorical data. so we will consider this as mode
reviews_per_month is numerical data we will replace with mean/median

"""

print('mode=',df['last_review'].mode())

df['last_review'].fillna('2019-06-23',inplace=True)

df['last_review'].value_counts()

print('Mean=',df['reviews_per_month'].mean())
print('Median=',df['reviews_per_month'].median())

"""As not much of difference we can go with mean

"""

df['reviews_per_month'].fillna(df['reviews_per_month'].mean(),inplace=True)

df.isnull().sum()

"""2. Handling Outliers"""

sns.boxplot(data=df, x="price")

plt.figure(figsize=(20, 19))
sns.boxplot(data=df, x="price", y="host_id")
plt.show()

plt.figure(figsize=(25, 19))
sns.boxplot(data=df, x="price", y="room_type",whis=10)
plt.show()

len(df[(df["room_type"]=='Private room') & (df['price']>500)])

len(df[(df["room_type"]=='Entire home/apt') & (df['price']>1250)])

len(df[(df["room_type"]=='Shared room') & (df['price']>425)])

index_names=df[(df["room_type"]=='Private room') & (df['price']>500)].index

df.drop(index_names,inplace=True)

index_names=df[(df["room_type"]=='Entire home/apt') & (df['price']>1250)].index

df.drop(index_names,inplace=True)

index_names=df[(df["room_type"]=='Shared room') & (df['price']>425)].index

df.drop(index_names,inplace=True)

plt.figure(figsize=(25, 19))
sns.boxplot(data=df, x="price", y="room_type",whis=10)
plt.show()

plt.figure(figsize=(40, 19))
sns.boxplot(data=df, x="price", y="reviews_per_month",whis=5)
plt.show()

plt.figure(figsize=(20, 19))
sns.boxplot(data=df, x="price", y="neighbourhood_group",whis=7)
plt.show()

plt.figure(figsize=(20, 19))
sns.boxplot(data=df, x="price", y="minimum_nights",whis=5)
plt.show()

plt.figure(figsize=(20, 19))
sns.boxplot(data=df, x="price", y="number_of_reviews",whis=5)
plt.show()

plt.figure(figsize=(20, 19))
sns.boxplot(data=df, x="price", y="calculated_host_listings_count",whis=5)
plt.show()

plt.figure(figsize=(20, 19))
sns.boxplot(data=df, x="price", y="availability_365",whis=5)
plt.show()

"""We have removed outliers based on room type Price

Handling Categorical data
"""

# Seperate categorical and numerical columns
df_cat = df[['host_id','neighbourhood_group','room_type','last_review']]
df_num = df.select_dtypes(["float64",'int64'])

df_cat

df_num.drop('host_id',axis=1,inplace=True)

# LabelEncoder
from sklearn.preprocessing import LabelEncoder

for col in df_cat:
  le = LabelEncoder()
  df_cat[col] = le.fit_transform(df_cat[col])
print(df_cat.head())

#Reducing skewness
from scipy.stats import skew

for col in df_num:
  try:
    print(col, "=", skew(df_num[col]))
    sns.distplot(df_num[col])
    plt.show()
  except:
    pass
  finally:
    print("************************************************")
# skew value is < -0.5 or > 0.5

# High Skewness -   minimum_nights ,number_of_reviews , reviews_per_month ,calculated_host_listings_count,availability_365

df.corr()

min(df['minimum_nights'])

max(df['minimum_nights'])

skewed_data = np.sqrt(df_num['minimum_nights'])
skew(skewed_data)

skewed_data = np.cbrt(df_num['minimum_nights'])
skew(skewed_data)

skewed_data = np.log10(df_num['minimum_nights'])
skew(skewed_data)

min(df['number_of_reviews'] )

max(df['number_of_reviews'] )

skewed_data =np.sqrt(df_num['number_of_reviews'])
skew(skewed_data)

skewed_data =np.cbrt(df_num['number_of_reviews'])
skew(skewed_data)

skewed_data =np.sqrt(np.sqrt(df_num['number_of_reviews']))
skew(skewed_data)

min(df['reviews_per_month'])

max(df['reviews_per_month'])

skewed_data =np.sqrt(df_num['reviews_per_month'])
skew(skewed_data)

skewed_data =np.cbrt(df_num['reviews_per_month'])
skew(skewed_data)

min(df['calculated_host_listings_count'])

max(df['calculated_host_listings_count'])

skewed_data =np.cbrt(df_num['calculated_host_listings_count'])
skew(skewed_data)

skewed_data =np.log10(df_num['calculated_host_listings_count'])
skew(skewed_data)

skewed_data =np.cbrt(np.cbrt(df_num['calculated_host_listings_count']))
skew(skewed_data)

min(df['availability_365'])

max(df['availability_365'])

skewed_data =np.sqrt(df_num['availability_365'])
skew(skewed_data)

df_num['minimum_nights'] = np.log10(df_num['minimum_nights'])
df_num['number_of_reviews'] = np.sqrt(np.sqrt(df_num['number_of_reviews']))
df_num['reviews_per_month']  = np.cbrt(df_num['reviews_per_month'])
df_num['calculated_host_listings_count']  = np.log10(df_num['calculated_host_listings_count'])
df_num['availability_365']  =np.sqrt(df_num['availability_365'])

df_num.head()



"""5. Scaling of data"""

from sklearn.preprocessing import MinMaxScaler

for col in df_num.drop('price',axis=1):
  ss=MinMaxScaler()
  df_num[col]=ss.fit_transform(df_num[[col]])
df_num.head()

# merge data frames to make it complete
new_df = pd.concat([df_num, df_cat], axis=1)

plt.figure(figsize=(10,9))
sns.heatmap(new_df.corr(),annot=True)

"""AS seen there is not much of high correlation between dependent and independent variables.
We will directly go with polynomial regression
"""

#Linear Regression Task
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score

from sklearn.preprocessing import PolynomialFeatures

x = new_df.drop(['price'], axis=1)
y = new_df['price']

pf = PolynomialFeatures(2)
x_degree_2= pf.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_degree_2,y,test_size=0.3)
lr = LinearRegression()
lr.fit(x_train,y_train)
y_train_hat = lr.predict(x_train)
print("Bias = ",r2_score(y_train,y_train_hat))
y_test_hat = lr.predict(x_test)
print("Variance = ",r2_score(y_test,y_test_hat))

"""Polynomial regression with degree 2 is just 34%"""

from sklearn.tree import DecisionTreeRegressor

dt_g = DecisionTreeRegressor(max_depth=5)
dt_g.fit(x_train,y_train)
print("Bias = ",dt_g.score(x_train,y_train))
print("Variance = ",dt_g.score(x_test,y_test))

"""Decision Tree is giving 34%

As score is very less lets consider Feature selection -Annova test
"""

from sklearn.feature_selection import f_regression

from sklearn.feature_selection import SelectKBest
annova = SelectKBest(score_func=f_regression,k=2)
annova.fit(x_train,y_train)
x_train_annova = annova.transform(x_train)
x_test_annova = annova.transform(x_test)

from sklearn.tree import DecisionTreeRegressor

dt = DecisionTreeRegressor(max_depth=5)
dt.fit(x_train_annova,y_train)
print("Bias = ",dt.score(x_train_annova,y_train))
print("Variance = ",dt.score(x_test_annova,y_test))

"""With annova test the score is too less.its just 24%
So let consider ensembling technique
"""

from sklearn.ensemble import VotingRegressor

voting = VotingRegressor(estimators=[("polynomial regression", lr), 
                                      ("Decision Tree -gini", dt_g)])

voting.fit(x_train, y_train)
voting.score(x_test, y_test)

"""So we can say that we can use ensembling technique with models as Polynomial Regression and Decision Tree together.
Score 36%
"""

