# -*- coding: utf-8 -*-
"""Airline_Satsifaction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fNAQh-L-sSX8_vIpUdvfZ9LEjX7nmYTS
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_train=pd.read_csv('train.csv')
df_test=pd.read_csv('test.csv')

df_train.shape

df_train.info()

"""There are 2 columns Unnamed,Id which are not required we will remove the columns
Also, there is satisfaction categorical column which is target column so removing it also.
"""

df_train.drop(['Unnamed: 0','id','satisfaction'],axis=1,inplace=True)

df_train.info()

df_train.isnull().sum()

"""Handling missing values for Arrival delay in minutes

"""

print('Mean=',df_train['Arrival Delay in Minutes'].mean())
print('Median=',df_train['Arrival Delay in Minutes'].median())

"""As there is much difference between mean and median we will go with median"""

df_train['Arrival Delay in Minutes'].fillna(df_train['Arrival Delay in Minutes'].median(),inplace=True)

df_train.isnull().sum()

#Analysis
sns.countplot(df_train['Target'])
plt.show()

sns.countplot(df_train['Target'],hue=df_train['Gender'])
plt.show()

sns.countplot(df_train['Target'],hue=df_train['Type of Travel'])
plt.show()

sns.countplot(df_train['Target'],hue=df_train['Class'])
plt.show()

sns.countplot(df_train['Target'],hue=df_train['Cleanliness'])
plt.show()

"""As per analysis:
1. Business Travel satisfied
2. Business Class more satisfied
3. Economy class less satissfied

2. Handling Outliers
"""

plt.figure(figsize=(10,7))
sns.boxplot(data=df_train,x='Arrival Delay in Minutes',whis=10)
plt.show()

plt.figure(figsize=(10,7))
sns.boxplot(data=df_train,x='Departure Delay in Minutes',whis=10)
plt.show()

"""Post Data analysis as outliers are more Its decided not to remove them for Arrival/Departure delay


"""

#Handling Categorical data 
#categorical columns -- Gender, Customer Type, Type of Travel, Class, Inflight wifi service, Departure/Arrival time convenient, Ease of Online booking,
 #Gate location,Food and drink, Online boarding,Seat comfort, ...
#numerical columns   -- Age, Flight Distance, Departure Delay in Minutes, Arrival Delay in Minutes

df_num_train=df_train[['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']]
df_cat_train=df_train.copy()
df_cat_train.drop(['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes'],axis=1,inplace=True)

"""Handling skewness"""

from scipy.stats import skew

for col in df_num_train:
  try:
    print(col,"=",skew(df_num_train[col]))
    sns.distplot(df_num_train[col])
    plt.show()
  except:
    pass
  finally:
    print("**********************************************")

"""As seen above we can see there is positive skewness for flight Distance,DepartureDelay in minutes,ArrivalDelay in minutes"""

plt.figure(figsize=(12,12))
sns.heatmap(df_train.corr(),annot=True)

print(min(df_train['Flight Distance']))
print(max(df_train['Flight Distance']))

print(min(df_train['Departure Delay in Minutes']))
print(max(df_train['Departure Delay in Minutes']))

print(min(df_train['Arrival Delay in Minutes']))
print(max(df_train['Arrival Delay in Minutes']))

skewed_data = np.sqrt(df_num_train['Flight Distance'])
skew(skewed_data)

skewed_data = np.sqrt(df_num_train['Departure Delay in Minutes'])
skew(skewed_data)

skewed_data = np.cbrt(df_num_train['Departure Delay in Minutes'])
skew(skewed_data)

skewed_data = np.sqrt(np.sqrt(df_num_train['Departure Delay in Minutes']))
skew(skewed_data)

skewed_data = np.sqrt(np.cbrt(df_num_train['Departure Delay in Minutes']))
skew(skewed_data)

skewed_data = np.sqrt(df_num_train['Arrival Delay in Minutes'])
skew(skewed_data)

skewed_data = np.cbrt(df_num_train['Arrival Delay in Minutes'])
skew(skewed_data)

skewed_data =np.sqrt(np.sqrt(df_num_train['Arrival Delay in Minutes']))
skew(skewed_data)

skewed_data = np.sqrt(np.cbrt(df_num_train['Arrival Delay in Minutes']))
skew(skewed_data)

"""As per analysis we will reduce skewness as follows:
1. Flight Distance - square root
2. Departure Delay in minutes - Pentaroot
3. Arrival Delay in minutes - Pentaroot
"""

df_num_train['Flight Distance'] =  np.sqrt(df_num_train['Flight Distance'])
df_num_train['Departure Delay in Minutes'] = np.sqrt(np.cbrt(df_num_train['Departure Delay in Minutes']))
df_num_train['Arrival Delay in Minutes'] = np.sqrt(np.cbrt(df_num_train['Arrival Delay in Minutes']))

df_num_train.head()

from sklearn.preprocessing import MinMaxScaler
for col in df_num_train:
  mm = MinMaxScaler()
  df_num_train[col] = mm.fit_transform(df_num_train[[col]])
df_num_train.head()

"""Handling Categorical Data"""

from sklearn.preprocessing import LabelEncoder

for col in df_cat_train:
  le=LabelEncoder()
  df_cat_train[col]=le.fit_transform(df_cat_train[col])
print(df_cat_train.head())

#merge dataframes to make it complete
df_new_train=pd.concat([df_num_train,df_cat_train],axis=1)

df_new_train.head()

"""This is completion of EDA for training Data.
The same has to be done for test data
"""

df_test.shape

df_test.info()

"""There are 2 columns Unnamed,Id which are not required we will remove the columns
Also, there is satisfaction categorical column which is target column so removing it also.

"""

df_test.drop(['Unnamed: 0','id','satisfaction'],axis=1,inplace=True)

df_test.info()

df_test.isnull().sum()

"""Handling missing values for Arrival delay in minutes"""

print('Mean=',df_test['Arrival Delay in Minutes'].mean())
print('Median=',df_test['Arrival Delay in Minutes'].median())

"""As there is much difference between mean and median we will go with median"""

df_test['Arrival Delay in Minutes'].fillna(df_test['Arrival Delay in Minutes'].median(),inplace=True)

df_test.isnull().sum()

"""Handling Outliers"""

plt.figure(figsize=(10,7))
sns.boxplot(data=df_test,x='Arrival Delay in Minutes',whis=10)
plt.show()

plt.figure(figsize=(10,7))
sns.boxplot(data=df_test,x='Departure Delay in Minutes',whis=10)
plt.show()

"""Post Data analysis as outliers are more Its decided not to remove them for Arrival/Departure delay"""

df_num_test=df_test[['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']]
df_cat_test=df_test.copy()
df_cat_test.drop(['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes'],axis=1,inplace=True)

"""Handling skewness"""

from scipy.stats import skew

for col in df_num_test:
  try:
    print(col,"=",skew(df_num_test[col]))
    sns.distplot(df_num_test[col])
    plt.show()
  except:
    pass
  finally:
    print("**********************************************")

"""As seen above we can see there is positive skewness for flight Distance,DepartureDelay in minutes,ArrivalDelay in minutes
Negative skewness for Age
"""

plt.figure(figsize=(12,12))
sns.heatmap(df_test.corr(),annot=True)

print(min(df_test['Age']))
print(max(df_test['Age']))

skewed_data = np.square(df_num_test['Age'])
skew(skewed_data)

skewed_data = np.power(df_num_test['Age'],3)
skew(skewed_data)

print(min(df_test['Flight Distance']))
print(max(df_test['Flight Distance']))

print(min(df_test['Departure Delay in Minutes']))
print(max(df_test['Departure Delay in Minutes']))

print(min(df_test['Arrival Delay in Minutes']))
print(max(df_test['Arrival Delay in Minutes']))

skewed_data = np.sqrt(df_num_test['Flight Distance'])
skew(skewed_data)

skewed_data = np.sqrt(df_num_test['Departure Delay in Minutes'])
skew(skewed_data)

skewed_data = np.cbrt(df_num_test['Departure Delay in Minutes'])
skew(skewed_data)

skewed_data = np.sqrt(np.sqrt(df_num_test['Departure Delay in Minutes']))
skew(skewed_data)

skewed_data = np.sqrt(np.cbrt(df_num_test['Departure Delay in Minutes']))
skew(skewed_data)

skewed_data = np.sqrt(df_num_test['Arrival Delay in Minutes'])
skew(skewed_data)

skewed_data =np.sqrt(np.sqrt(df_num_test['Arrival Delay in Minutes']))
skew(skewed_data)

skewed_data = np.sqrt(np.cbrt(df_num_test['Arrival Delay in Minutes']))
skew(skewed_data)

"""As per analysis we will reduce skewness as follows:
1. Flight Distance - square root
2. Departure Delay in minutes - Pentaroot
3. Arrival Delay in minutes - Pentaroot
4. Age-Square
"""

df_num_test['Age']= np.square(df_num_test['Age'])
df_num_test['Flight Distance'] =  np.sqrt(df_num_test['Flight Distance'])
df_num_test['Departure Delay in Minutes'] = np.sqrt(np.cbrt(df_num_test['Departure Delay in Minutes']))
df_num_test['Arrival Delay in Minutes'] = np.sqrt(np.cbrt(df_num_test['Arrival Delay in Minutes']))
df_num_test.head()

from sklearn.preprocessing import MinMaxScaler

for col in df_num_test:
  mm = MinMaxScaler()
  df_num_test[col] = mm.fit_transform(df_num_test[[col]])
df_num_test.head()

"""Handling Categorical Data

"""

from sklearn.preprocessing import LabelEncoder
for col in df_cat_test:
  le=LabelEncoder()
  df_cat_test[col]=le.fit_transform(df_cat_test[col])
print(df_cat_test.head())

#merge dataframes to make it complete
df_new_test=pd.concat([df_num_test,df_cat_test],axis=1)
df_new_test.head()

"""Logistic Regression Algorithm

"""

x_train = df_new_train.drop(['Target','Arrival Delay in Minutes'],axis=1)
y_train = df_new_train['Target']
x_test=df_new_test.drop(['Target','Arrival Delay in Minutes'],axis=1)
y_test=df_new_test['Target']

from sklearn.linear_model import LogisticRegression

logr = LogisticRegression()
logr.fit(x_train,y_train)

y_hat = logr.predict(x_test)

from sklearn.metrics import confusion_matrix
tn, fp, fn, tp = confusion_matrix(y_test,y_hat).ravel()

print(tp,",",fn)
print(fp,",",tn)

from sklearn.metrics import(accuracy_score, recall_score, precision_score, f1_score)

print("Accuracy Score: ",accuracy_score(y_test, y_hat))
print("Recall Score: ",recall_score(y_test, y_hat))
print("Precision Score: ",precision_score(y_test, y_hat))
print("F1 Score: ",f1_score(y_test, y_hat))



"""ROC-AUC Curve"""

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test,y_hat))

from sklearn.metrics import roc_curve
fpr, tpr, threshold = roc_curve(y_test,y_hat)

plt.plot(fpr,tpr,'r-',label="Logistic Model")
plt.xlabel("False positive rate")
plt.ylabel("True positive rate")
plt.legend()
plt.show()

"""As seen it is giving 86% .It is a good model

Decision Tree Algorithm
"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(x_train,y_train)
print("Bias = ",dt.score(x_train,y_train))
print("Variance = ",dt.score(x_test,y_test))

from sklearn import tree

dt_e = DecisionTreeClassifier(criterion="entropy")
dt_e.fit(x_train,y_train)
print("Bias = ",dt_e.score(x_train,y_train))
print("Variance = ",dt_e.score(x_test,y_test))

# As Decision Tree is overfitting we will prune it
dt_e = DecisionTreeClassifier(criterion='entropy',max_depth=3)
dt_e.fit(x_train, y_train)
print("Bias = ",dt_e.score(x_train,y_train))
print("Variance = ",dt_e.score(x_test,y_test))

# As Decision Tree is overfitting we will prune it
dt_g = DecisionTreeClassifier(criterion='gini',max_depth=3)
dt_g.fit(x_train, y_train)
print("Bias = ",dt_g.score(x_train,y_train))
print("Variance = ",dt_g.score(x_test,y_test))

"""Decision Tree Algorithm gives 88% score.

KNN Algorithm
"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn.score(x_test, y_test)

"""KNN gives 92 % score

SVM Algorithm
"""

from sklearn.svm import LinearSVC, SVC

svc = LinearSVC()
svc.fit(x_train, y_train)
svc.score(x_test, y_test)

svc_poly = SVC(kernel="poly")
svc_poly.fit(x_train, y_train)
svc_poly.score(x_test, y_test)

svc_rbf = SVC(kernel="rbf")
svc_rbf.fit(x_train, y_train)
svc_rbf.score(x_test, y_test)

"""As we can see that KNN, SVC-Kernel  =poly,SVC-kernel='rbf' all 3 scores are nearby 92,93,94 we will implement ensembling technique of Naive aggregation - Hard voting"""

from sklearn.ensemble import VotingClassifier

voting = VotingClassifier(estimators=[("KNN", knn), 
                                      ("SVM Poly", svc_poly),
                                      ("SVM RBF", svc_rbf)], voting="hard")
voting.fit(x_train, y_train)
voting.score(x_test, y_test)

from sklearn.feature_selection import f_regression #annova
from sklearn.feature_selection import SelectKBest
annova=SelectKBest(score_func=f_regression,k=20)
annova.fit(x_train,y_train)
x_train_annova=annova.transform(x_train)
x_test_annova=annova.transform(x_test)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train_annova, y_train)
knn.score(x_test_annova, y_test)

svc_poly = SVC(kernel="poly")
svc_poly.fit(x_train_annova, y_train)
svc_poly.score(x_test_annova, y_test)

svc_rbf = SVC(kernel="rbf")
svc_rbf.fit(x_train_annova, y_train)
svc_rbf.score(x_test_annova, y_test)

voting = VotingClassifier(estimators=[("KNN", knn), 
                                      ("SVM Poly", svc_poly),
                                      ("SVM RBF", svc_rbf)], voting="hard")
voting.fit(x_train_annova, y_train)
voting.score(x_test_annova, y_test)

"""As seen we can say we can go with KNN algorithm,SVM poly and svn kernel rbf with ensembling technique hard voting"""

scores_df=pd.DataFrame(annova.scores_)
columns_df = pd.DataFrame(x_train.columns)

featurescore=pd.concat([columns_df,scores_df],axis=1)
featurescore.columns=['Feature Name','Score']

featurescore

